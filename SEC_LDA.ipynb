{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code attempts to extract the risk factor section of many raw text html 10Ks/10Qs and then explores the tractability of a simple LDA analysis on these documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "from timeit import default_timer as timer\n",
    "import pickle\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "#makes a pandas cell clickable\n",
    "def cell_clickable(val):\n",
    "    return '<a href=\"{}\">{}</a>'.format(val,val)\n",
    "#makes pandas dataframe's col 'htmURL' clickable from the ipython notebook\n",
    "def make_clickable(df):\n",
    "    if len(df)>100: df=df.sample(n=100)  #safeguard against displaying large dataframes\n",
    "    return df.style.format({'htmURL': cell_clickable})\n",
    "\n",
    "#description: attempts to remove html code from a raw text html file, leaving mostly readable text\n",
    "#input: raw text html string\n",
    "#output: lowercase readable text string\n",
    "def clean_html(text):\n",
    "    #tag removal first pass\n",
    "    text = re.sub('<.*?>', ' ', text) \n",
    "    #attempts to standardize characters, may or may not work perfectly\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore') \n",
    "    #removes html special chars, non-printable ascii characters,\n",
    "    #string literals, and a second pass of tag removal\n",
    "    text = re.sub('&.*?;|[^\\x00-\\x7F]+|\\r|\\n|\\a|\\b|\\f|\\t|\\v|<.*?>', ' ',text)\n",
    "    text = re.sub(' +', ' ', text) #removes extra whitespace\n",
    "    return text.lower().strip() #returns lower case\n",
    "\n",
    "#description: takes text string and removes some 'unimportant' portions\n",
    "#input: text string\n",
    "#output: reduced text string\n",
    "def clean(text):\n",
    "    text = re.sub('\\d', ' ', text) #removes numbers\n",
    "    text = re.sub(r'[?|!|\\'|\\\"|#|.|,|)|(|\\|/|$|:|;|&|^|%|@|_|-]', '', text)#removes punctuation\n",
    "    text = re.sub('(?:^| )\\w(?:$| )', ' ',text)#removes single characters\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95506, 12)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complete = pd.read_csv('df_coords.csv')\n",
    "df_reduced = df_complete.filter(items=['CIK','COMN','DATE','FILE','FORM','fdate','formt','ftime','htmLOC','htmURL','locURL','period'])\n",
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mixed sample of randomly selected 10Ks and 10Qs pulled from the SEC website. Because this data is from another project the sample data's dates of filing will be skewed towards 2010-2015, but this shouldn't matter much for a short demo. For now lets focus on a smaller size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556ed\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >CIK</th>        <th class=\"col_heading level0 col1\" >COMN</th>        <th class=\"col_heading level0 col2\" >DATE</th>        <th class=\"col_heading level0 col3\" >FILE</th>        <th class=\"col_heading level0 col4\" >FORM</th>        <th class=\"col_heading level0 col5\" >fdate</th>        <th class=\"col_heading level0 col6\" >formt</th>        <th class=\"col_heading level0 col7\" >ftime</th>        <th class=\"col_heading level0 col8\" >htmLOC</th>        <th class=\"col_heading level0 col9\" >htmURL</th>        <th class=\"col_heading level0 col10\" >locURL</th>        <th class=\"col_heading level0 col11\" >period</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edlevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col0\" class=\"data row0 col0\" >318154</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col1\" class=\"data row0 col1\" >AMGEN INC</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col2\" class=\"data row0 col2\" >2012-08-08</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col3\" class=\"data row0 col3\" >edgar/data/318154/0001193125-12-344279.txt</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col4\" class=\"data row0 col4\" >10-Q</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col5\" class=\"data row0 col5\" >20120808</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col6\" class=\"data row0 col6\" >False</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col7\" class=\"data row0 col7\" >17:03:20</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col8\" class=\"data row0 col8\" >E:\\Data\\SEC\\HTML3\\31\\81\\54\\0001193125-12-344279.htm</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col9\" class=\"data row0 col9\" ><a href=\"https://www.sec.gov/Archives/edgar/data/318154/000119312512344279/d351286d10q.htm\">https://www.sec.gov/Archives/edgar/data/318154/000119312512344279/d351286d10q.htm</a></td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col10\" class=\"data row0 col10\" >E:\\Data\\SEC\\HTML\\b\\75\\318154_61</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow0_col11\" class=\"data row0 col11\" >20120630</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edlevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col0\" class=\"data row1 col0\" >870826</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col1\" class=\"data row1 col1\" >VALUEVISION MEDIA INC</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col2\" class=\"data row1 col2\" >2012-09-04</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col3\" class=\"data row1 col3\" >edgar/data/870826/0000870826-12-000010.txt</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col4\" class=\"data row1 col4\" >10-Q</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col5\" class=\"data row1 col5\" >20120831</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col6\" class=\"data row1 col6\" >False</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col7\" class=\"data row1 col7\" >17:40:38</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col8\" class=\"data row1 col8\" >E:\\Data\\SEC\\HTML3\\87\\8\\26\\0000870826-12-000010.htm</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col9\" class=\"data row1 col9\" ><a href=\"https://www.sec.gov/Archives/edgar/data/870826/000087082612000010/vvtv10q07282012.htm\">https://www.sec.gov/Archives/edgar/data/870826/000087082612000010/vvtv10q07282012.htm</a></td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col10\" class=\"data row1 col10\" >E:\\Data\\SEC\\HTML\\d\\63\\870826_31</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow1_col11\" class=\"data row1 col11\" >20120728</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edlevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col0\" class=\"data row2 col0\" >1487101</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col1\" class=\"data row2 col1\" >KEYW HOLDING CORP</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col2\" class=\"data row2 col2\" >2011-08-04</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col3\" class=\"data row2 col3\" >edgar/data/1487101/0001144204-11-043965.txt</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col4\" class=\"data row2 col4\" >10-Q</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col5\" class=\"data row2 col5\" >20110804</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col6\" class=\"data row2 col6\" >False</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col7\" class=\"data row2 col7\" >16:05:56</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col8\" class=\"data row2 col8\" >E:\\Data\\SEC\\HTML3\\148\\71\\1\\0001144204-11-043965.htm</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col9\" class=\"data row2 col9\" ><a href=\"https://www.sec.gov/Archives/edgar/data/1487101/000114420411043965/v230285_10q.htm\">https://www.sec.gov/Archives/edgar/data/1487101/000114420411043965/v230285_10q.htm</a></td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col10\" class=\"data row2 col10\" >E:\\Data\\SEC\\HTML\\i\\99\\1487101_88</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow2_col11\" class=\"data row2 col11\" >20110630</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edlevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col0\" class=\"data row3 col0\" >1091325</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col1\" class=\"data row3 col1\" >CHINA YIDA HOLDING, CO.</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col2\" class=\"data row3 col2\" >2012-05-11</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col3\" class=\"data row3 col3\" >edgar/data/1091325/0001213900-12-002399.txt</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col4\" class=\"data row3 col4\" >10-Q</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col5\" class=\"data row3 col5\" >20120511</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col6\" class=\"data row3 col6\" >False</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col7\" class=\"data row3 col7\" >08:53:52</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col8\" class=\"data row3 col8\" >E:\\Data\\SEC\\HTML3\\109\\13\\25\\0001213900-12-002399.htm</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col9\" class=\"data row3 col9\" ><a href=\"https://www.sec.gov/Archives/edgar/data/1091325/000121390012002399/f10q0312_chinayida.htm\">https://www.sec.gov/Archives/edgar/data/1091325/000121390012002399/f10q0312_chinayida.htm</a></td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col10\" class=\"data row3 col10\" >E:\\Data\\SEC\\HTML\\l\\11\\1091325_11</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow3_col11\" class=\"data row3 col11\" >20120331</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edlevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col0\" class=\"data row4 col0\" >1378706</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col1\" class=\"data row4 col1\" >AMERICAN DG ENERGY INC</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col2\" class=\"data row4 col2\" >2014-04-09</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col3\" class=\"data row4 col3\" >edgar/data/1378706/0001378706-14-000002.txt</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col4\" class=\"data row4 col4\" >10-K</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col5\" class=\"data row4 col5\" >20140409</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col6\" class=\"data row4 col6\" >True</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col7\" class=\"data row4 col7\" >13:54:06</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col8\" class=\"data row4 col8\" >E:\\Data\\SEC\\HTML3\\137\\87\\6\\0001378706-14-000002.htm</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col9\" class=\"data row4 col9\" ><a href=\"https://www.sec.gov/Archives/edgar/data/1378706/000137870614000002/adge-20131231x10k.htm\">https://www.sec.gov/Archives/edgar/data/1378706/000137870614000002/adge-20131231x10k.htm</a></td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col10\" class=\"data row4 col10\" >E:\\Data\\SEC\\HTML\\i\\34\\1378706_74</td>\n",
       "                        <td id=\"T_0b9d426e_8aed_11e9_90a7_ac220bc556edrow4_col11\" class=\"data row4 col11\" >20131231</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2bb79bc0908>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_reduced.sample(n=1000).reset_index(drop=True)\n",
    "make_clickable(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<DOCUMENT>\\n<TYPE>10-Q\\n<SEQUENCE>1\\n<FILENAME>d351286d10q.htm\\n<DESCRIPTION>FORM 10-Q\\n<TEXT>\\n<HTML><HEAD>\\n<TITLE>Form 10-Q</TITLE>\\n</HEAD>\\n <BODY BGCOLOR=\"WHITE\">\\n<h5 align=\"left\"><a href=\"#toc\">Table of Contents</a></h5>\\n\\n <P STYLE=\"line-height:3px;margin-top:0px;margin-bottom:0px;border-bottom:3pt solid #000000\">&nbsp;</P>\\n<P STYLE=\"line-height:3px;margin-top:0px;margin-bottom:2px;border-bottom:0.5pt solid #000000\">&nbsp;</P> <P STYLE=\"margin-top:12px;margin-bottom:0px\" ALIGN=\"center\"><FONT STYLE=\"font-family:Times New Roman\" SIZE=\"5\"><B>UNITED STATES </B></FONT></P>\\n<P STYLE=\"margin-top:0px;margin-bottom:0px\" ALIGN=\"center\"><FONT STYLE=\"font-family:Times New Roman\" SIZE=\"5\"><B>SECURITIES AND EXCHANGE COMMISSION </B></FONT></P> <P STYLE=\"margin-top:0px;margin-bottom:0px\" ALIGN=\"center\"><FONT\\nSTYLE=\"font-family:Times New Roman\" SIZE=\"3\"><B>Washington, D.C. 20549 </B></FONT></P> <P STYLE=\"margin-top:12px;margin-bottom:0px\" ALIGN=\"center\"><FONT STYLE=\"font-family:Times New Roman\" SIZE=\"5\">'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(df.loc[0,'htmLOC'], \"r\") as f: \n",
    "    docPage = f.read()\n",
    "\n",
    "docPage[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a typical html file, though this will need to be stripped down before any sort of analysis. Lots of punctuation and non-visible characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal now is to extract each risk factor section and store it in a list. To identify the risk factor section this code assumes that each file has a table of contents, and that the table of contents follows the naming convention discussed here https://www.sec.gov/fast-answers/answersreada10khtm.html. \n",
    "\n",
    "\n",
    "<br><br>\n",
    "TABLE OF CONTENTS\n",
    "\n",
    "Item 1.\n",
    "\t  \tBusiness \t   \t  \t1  \n",
    "\n",
    "Item 1A.\n",
    "\t  \tRisk Factors         5\n",
    "\n",
    "Item 1B.\n",
    "        Unresolved Staff Comments     9\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "From the SEC website it can be seen that the Risk Factors section should always be listed, and indexed by \"Item 1A.\". In addition the code takes advantage of the fact that most table of contents are uniquely linked to the appropriate page element. It first looks for the Risk Factors index and then its corresponding url tag, if successful it then continues looking for the next index's url tag; with the hope that the next index's url tag can be used to demarcate the end of the Risk Factor's section later. A typical table of contents may look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loop time: 33.321857795\n",
      "IO load time: 12.38193154700002\n",
      "Avg. iteration time: 0.033321857795\n",
      "674 entries were retrieved with a success rate of: 0.674\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "load_time = 0\n",
    "\n",
    "risk_factors_corpus = []\n",
    "cleaner = re.compile('&.*?;|\\xa0|\\r|\\n|\\a|\\b|\\f|\\t|\\v')\n",
    "for docLoc in df['htmLOC'].tolist():\n",
    "    start_load = timer()\n",
    "    with open(docLoc, \"r\") as f: \n",
    "        docPage = f.read()\n",
    "    end_load = timer()\n",
    "    load_time += (end_load - start_load)\n",
    "    \n",
    "\n",
    "    docPage = docPage.lower()\n",
    "    docPage = cleaner.sub(' ',docPage)\n",
    "\n",
    "    \n",
    "    item_1a_index = docPage.find('item 1a') \n",
    "    risk_toc_index = docPage[item_1a_index:].find('risk factors')+item_1a_index\n",
    "    href_index = docPage[:risk_toc_index].rfind('href=\"#') #looks for first link before 'risk factors'\n",
    "    if(href_index <0): continue #skips 10k/10qs that don't have hotlinked table of contents\n",
    "    left_tag_index = href_index+7\n",
    "    right_tag_index = docPage[left_tag_index:].find('\"')+left_tag_index \n",
    "    risk_element_tag = docPage[left_tag_index:right_tag_index] #isolated risk factor tag\n",
    "    if len(risk_element_tag)==0: continue #this likely never happens\n",
    "\n",
    "    next_element_tag = \"\"\n",
    "    post_risk_toc_index = -1\n",
    "    risk_start_index = -1\n",
    "    risk_end_index = -1\n",
    "    next_hrefs=[i.end() for i in re.finditer('href=\"#', docPage[risk_toc_index:risk_toc_index+2500])] #looks for links after 'risk factors'\n",
    "    for i in next_hrefs:\n",
    "        left_tag_index = risk_toc_index+i\n",
    "        right_tag_index = docPage[left_tag_index:].find('\"')+left_tag_index #looks for end of element tag\n",
    "        if(right_tag_index < left_tag_index): continue  #this also likely never happens\n",
    "        next_element_tag = docPage[left_tag_index:right_tag_index]\n",
    "        #this checks to see if the next url links beyond the risk section\n",
    "        #this looks ineffecient but is needed because sometimes the links are out of order\n",
    "        if (next_element_tag != risk_element_tag) and (next_element_tag != \"\"): \n",
    "            post_risk_toc_index = right_tag_index\n",
    "            risk_start_index = docPage[post_risk_toc_index:].find('<a name=\"' + risk_element_tag  +'\">')+post_risk_toc_index\n",
    "            risk_end_index = docPage[risk_start_index:].find('<a name=\"' + next_element_tag  +'\">')+risk_start_index\n",
    "            if(risk_start_index < risk_end_index): break #the next unique url after the risk factors toc entry that links beyond where the risk factors url links to has been found\n",
    "    if(risk_start_index >= risk_end_index):continue            \n",
    "\n",
    "    risk_factors = docPage[risk_start_index:risk_end_index]\n",
    "    risk_factors = risk_factors[risk_factors.find(\">\")+1:risk_factors.rfind(\"<\")]\n",
    "    risk_factors = clean_html(risk_factors)\n",
    "    \n",
    "    risk_factors_corpus.append(risk_factors)\n",
    "    \n",
    "    \n",
    "end = timer()\n",
    "print('Total loop time: ' + str(end - start))\n",
    "print('IO load time: ' + str(load_time))\n",
    "print('Avg. iteration time: ' + str(((end - start)/len(df))))\n",
    "print(str(len(risk_factors_corpus))+\" entries were retrieved with a success rate of: \" + str(len(risk_factors_corpus)/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This obviously doesn't work for every 10K/10Q but the well labelled and linked table of contents is a common enough convetion that the code has pulled a section for ~60% of the documents. Also it's worth noting this isn't particularly fast, and it's likely some of the regex cleaning portions could be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item 1a. risk factors this report and other documents we file with the sec contain forward-looking statements that are based on current expectations, estimates, forecasts and projections about us, our future performance, our business, our beliefs and our management s assumptions. these statements are not guarantees of future performance and involve certain risks, uncertainties and assumptions that are difficult to predict. you should carefully consider the risks and uncertainties facing our busi ... erations. further, additional risks not currently known to us or that we currently believe are immaterial may in the future materially and adversely affect our business, operations, liquidity and stock price. there are no material updates from the risk factors previously disclosed in part i, item 1a, of our annual report on form 10-k for the fiscal year ended december 31, 2011, and in part ii, item ia, of our quarterly report on form 10-q for the period ended march 31, 2012. 36 table of contents'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'item 1a. risk factors in addition to the other information contained in this annual report on form 10-k, you should consider the following risk factors in evaluating our results of operations, financial condition, business and operations or an investment in the shares of our company. the risk factors described in this section have been separated into four groups: risks that relate to the competition we face and the technology used in our businesses; risks that relate to our operating in overseas ...  it would be to bring similar claims against a u.s. company in a u.s. court. in particular, english law significantly limits the circumstances under which shareholders of english companies may bring derivative actions. under english law generally, only the company can be the proper plaintiff in proceedings in respect of wrongful acts committed against us. our articles of association provide for the exclusive jurisdiction of the english courts for shareholder lawsuits against us or our directors.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'item 1a. risk factors we operate in a business environment that involves numerous known and unknown risks and uncertainties that could have a materially adverse impact on our operations. the risks described below highlight some of the factors that have affected, and in the future could affect, our operations. you should carefully consider these risks. these risks are not the only ones we may face. additional risks and uncertainties of which we are unaware or that we currently deem immaterial als ... of our available cash, if any, for working capital and other general corporate purposes. any payment of future dividends will be at the discretion of our board of directors and will depend upon, among other things, our earnings, financial condition, capital requirements, debt levels, statutory and contractual restrictions applying to the payment of dividends and other considerations that our board of directors deems relevant. investors seeking cash dividends should not purchase our common stock.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    if len(risk_factors_corpus[i])<1000: continue\n",
    "    display(risk_factors_corpus[i][:500]+\" ... \"+risk_factors_corpus[i][-500:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There maybe some oddities somewhere but this looks good enough to continue. So we'll save the corpus, which so far has only had its tags/code/nonprintable characters removed and will need further preprocessing before analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg words per bag: 28495.32640949555\n",
      "Avg std of words per bag: 45241.50133404879\n",
      "Most words in one bag: 331174\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([len(x) for x in risk_factors_corpus])\n",
    "print(\"Avg words per bag: \" + str(np.average(lengths)))\n",
    "print(\"Avg std of words per bag: \" + str(np.std(lengths)))\n",
    "print(\"Most words in one bag: \" + str(max(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('risk_factors_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(risk_factors_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all pretty but standard but for preprocessing, I first remove all numbers, punctuation, and one/two letter words, I then use the prebuilt lemmatizer and stemmer from WordNet and Snowball respectively and store it as word-tokenized data. While each 10K/10Q still has its own bag of words, most of the contextual data been, for better or worse, has been stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Zach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Zach\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from gensim import models\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "numbers = ['twenty','thirty','forty','fifty','sixty','seventy','eighty','ninety',\n",
    "           'zero','one','two','three','four','five','six','seven','eight','nine',\n",
    "           'ten','eleven','twelve','thirteen','fourteen','fifteen','sixteen',\n",
    "           'seventeen','eighteen','nineteen','hundred','thousand','million','billion']\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stemmer = SnowballStemmer(\"english\") #better than porter\n",
    "stopwords = set([x for x in gensim.parsing.preprocessing.STOPWORDS] + numbers) #set has better for performance than list for checking membership\n",
    "\n",
    "#input: single body text string\n",
    "#output: tokenized/stemmed/lemmatized/filtered word string list\n",
    "def stem_lem_clean(text):\n",
    "    text = re.sub('\\d', ' ', text) #removes numbers\n",
    "    text = re.sub(r'[?|!|\\'|\\\"|#|.|,|)|(|\\|/|$|:|;|&|^|%|@|_|-]', '', text)#removes punctuation\n",
    "    text = re.sub(r'\\W*\\b\\w{1,2}\\b','',text)#removes words that are 1 and chars long\n",
    "    text = word_tokenize(text) #converts from paragraph to list\n",
    "    #stems and lemmatizes words, throws out stopwords\n",
    "    text = [stemmer.stem(WordNetLemmatizer().lemmatize(x, pos='v')) for  x in text if x not in stopwords]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loop time: 34.22308647900001\n",
      "Avg. iteration time: 0.05077609269881307\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "tokenized_risk = [stem_lem_clean(x) for x in risk_factors_corpus]\n",
    "end = timer()\n",
    "print('Total loop time: ' + str(end - start))\n",
    "print('Avg. iteration time: ' + str(((end - start)/len(risk_factors_corpus))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['item', 'risk', 'factor', 'report', 'document', 'file', 'sec', 'contain', 'forwardlook', 'statement', 'base', 'current', 'expect', 'estim', 'forecast', 'project', 'futur', 'perform', 'busi', 'belief']\n",
      "Avg words per bag: 2250.6958456973293\n",
      "Avg std of words per bag: 3550.065799249855\n",
      "Most words in (one bag: 26948\n"
     ]
    }
   ],
   "source": [
    "lengths = np.array([len(x) for x in tokenized_risk])\n",
    "print(tokenized_risk[0][:20])\n",
    "print(\"Avg words per bag: \" + str(np.average(lengths)))\n",
    "print(\"Avg std of words per bag: \" + str(np.std(lengths)))\n",
    "print(\"Most words in (one bag: \" + str(max(lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the effects of the stemming and lemmatization on the above sample. In addition word counts have been drastically reduced with the removal of stopwords/punctuation/numbers. This is a good thing because most of those words would just become noise later, without context they add almost no information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(5088 unique tokens: ['assumpt', 'base', 'belief', 'believ', 'care']...)\n"
     ]
    }
   ],
   "source": [
    "word2vec = gensim.corpora.Dictionary(tokenized_risk,prune_at=1000000) #this by itself creates numerical pairing for each unique word\n",
    "#internally keeps track of which bags of words have contributed to the dictionary\n",
    "#so a threshold can be set for word membership\n",
    "word2vec.filter_extremes(no_below=3, no_above=0.5)\n",
    "lengths = np.array([len(x) for x in tokenized_risk])\n",
    "print(word2vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even after setting membership thresholds, there's still a total of 5000 unique words over all of the corpus. One thing to improve on here is that word membership threshold should be done on a cross-section of firms. E.g, one company may really like a particular word, say \"motorhome\", because its central to their business. So that firm uses it a lot in their own 10K/10Qs over time but other firms may never mention it. It's unlikely the word \"motorhome\" would add much information but it would require a slightly more advanced dictionary implementation than gensim provides to deal with it. Now we add the word counts to the gensim's dictionary object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = [word2vec.doc2bow(x) for x in tokenized_risk] #creates a dictionary style counter for each bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that this counter dictionary is using numerical indexing and membership restrictions previously defined by the word2vec dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf scores are commonly used as a means of normalizing the data before applying many nlp algos but it's not really required for LDA work. It can be useful to help reduce dimensionality for extremely large datasets. (See page 11 of http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf) This dataset is already manageable though, so I will forgo it for the final LDA model. Nevertheless this is how you'd implement it with gensim if you wanted to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.2717032165200514),\n",
       " (1, 0.08996560184300338),\n",
       " (2, 0.3374346469439526),\n",
       " (3, 0.08223323604442176),\n",
       " (4, 0.0933074473205476),\n",
       " (5, 0.0722524688999295),\n",
       " (6, 0.07164114101988502),\n",
       " (7, 0.08463003575826447),\n",
       " (8, 0.09180853907078416),\n",
       " (9, 0.08497711828159607)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_modeller = models.TfidfModel(counter)\n",
    "tf_idf_matrix = tf_idf_modeller[counter]\n",
    "tf_idf_matrix[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's one list of tuples like the one above for each of the previously pulled 10K/10Qs. The 1st number in each tuple is a word's dictionary key, and the 2nd number is that word's bag specific tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loop time: 23.521939949\n"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "lda_modeller = gensim.models.LdaMulticore(counter, num_topics=12, id2word=word2vec, passes=10, per_word_topics=True, workers=4)\n",
    "end = timer()\n",
    "print('Total loop time: ' + str(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.027*\"product\" + 0.015*\"custom\" + 0.010*\"market\" + 0.010*\"cost\" + 0.009*\"sale\" + 0.009*\"signific\" + 0.009*\"price\" + 0.008*\"increas\" + 0.008*\"manufactur\" + 0.008*\"new\"\n",
      "Topic: 1 Word: 0.015*\"gas\" + 0.015*\"oil\" + 0.012*\"natur\" + 0.011*\"price\" + 0.011*\"product\" + 0.010*\"requir\" + 0.009*\"cost\" + 0.008*\"abil\" + 0.008*\"properti\" + 0.008*\"general\"\n",
      "Topic: 2 Word: 0.011*\"incom\" + 0.011*\"net\" + 0.011*\"asset\" + 0.010*\"cash\" + 0.009*\"month\" + 0.009*\"tax\" + 0.008*\"total\" + 0.008*\"march\" + 0.008*\"valu\" + 0.008*\"cost\"\n",
      "Topic: 3 Word: 0.028*\"stock\" + 0.024*\"product\" + 0.014*\"market\" + 0.011*\"develop\" + 0.011*\"prefer\" + 0.010*\"common\" + 0.009*\"seri\" + 0.009*\"price\" + 0.009*\"requir\" + 0.008*\"agreement\"\n",
      "Topic: 4 Word: 0.016*\"cost\" + 0.012*\"gas\" + 0.012*\"regul\" + 0.010*\"requir\" + 0.009*\"impact\" + 0.009*\"increas\" + 0.009*\"qep\" + 0.008*\"price\" + 0.008*\"natur\" + 0.007*\"product\"\n",
      "Topic: 5 Word: 0.020*\"servic\" + 0.009*\"market\" + 0.007*\"abil\" + 0.007*\"certain\" + 0.007*\"new\" + 0.007*\"requir\" + 0.007*\"stock\" + 0.006*\"agreement\" + 0.006*\"develop\" + 0.006*\"cash\"\n",
      "Topic: 6 Word: 0.031*\"product\" + 0.014*\"approv\" + 0.013*\"develop\" + 0.013*\"candid\" + 0.013*\"clinic\" + 0.012*\"patent\" + 0.011*\"market\" + 0.010*\"trial\" + 0.010*\"commerci\" + 0.010*\"regulatori\"\n",
      "Topic: 7 Word: 0.030*\"product\" + 0.011*\"requir\" + 0.010*\"market\" + 0.008*\"law\" + 0.007*\"regul\" + 0.007*\"new\" + 0.006*\"technolog\" + 0.006*\"signific\" + 0.006*\"cost\" + 0.006*\"state\"\n",
      "Topic: 8 Word: 0.018*\"custom\" + 0.016*\"product\" + 0.015*\"servic\" + 0.011*\"market\" + 0.009*\"requir\" + 0.008*\"revenu\" + 0.008*\"signific\" + 0.007*\"stock\" + 0.007*\"technolog\" + 0.007*\"increas\"\n",
      "Topic: 9 Word: 0.016*\"insur\" + 0.015*\"product\" + 0.009*\"market\" + 0.009*\"increas\" + 0.009*\"requir\" + 0.008*\"rat\" + 0.007*\"cost\" + 0.007*\"unit\" + 0.007*\"invest\" + 0.007*\"branch\"\n",
      "Topic: 10 Word: 0.022*\"loan\" + 0.011*\"invest\" + 0.011*\"asset\" + 0.011*\"market\" + 0.010*\"requir\" + 0.010*\"bank\" + 0.009*\"loss\" + 0.009*\"incom\" + 0.008*\"increas\" + 0.008*\"capit\"\n",
      "Topic: 11 Word: 0.012*\"invest\" + 0.011*\"fund\" + 0.009*\"market\" + 0.009*\"manag\" + 0.009*\"tax\" + 0.008*\"client\" + 0.007*\"increas\" + 0.007*\"requir\" + 0.007*\"signific\" + 0.007*\"servic\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_modeller.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.7088100156030634"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_modeller.log_perplexity(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here you could further optimize this likihood metric (more negative is better here) by changing the corpus filtering, the assumed number of topics, adding priori beliefs, tweaking how the calculations are iterated, and perhaps using n-grams. I think 2-grams could be really interesting in particular because in finance simple qualifiers can have a ton of informational context (e.g., \"increased revenue\", \"project decreases\", \"cash flow\"). In addition, training/test sets should be used if quantitative metrics are going to be used for evaluation. In particular, the above metric should not be evaluated on the training set (counter is the training set) for a serious optimization attempt. But with all that said, here's a model-based inference on the topic classification for one of the in-set documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.13253443), (3, 0.015137215), (8, 0.8439119)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_modeller[counter[8]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'item 1a. risk factors we operate in a business environment that involves numerous known and unknown risks and uncertainties that could have a materially adverse impact on our operations. the risks described below highlight some of the factors that have affected, and in the future could affect, our operations. you should carefully consider these risks. these risks are not the only ones we may face. additional risks and uncertainties of which we are unaware or that we currently deem immaterial also may become important factors that affect us. if any of the events or circumstances described in the followings risks occurs, our business, financial condition, results of operations, cash flows, or any combination of the foregoing, could be materially and adversely affected. our risks are described in detail below; however, the more significant risks we face include the following: if we are unable to attract new customers, or if our existing customers do not purchase additional products or ser...'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "risk_factors_corpus[8][:1000]+\"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the modelling is saying that Sciquest's fiscal ended 2012 10K ( https://www.sec.gov/Archives/edgar/data/1082526/000119312513098097/d444510d10k.htm#tx444510_3 ) should be classified as topic 8. Recall:\n",
    "<br><br>\n",
    "Topic: 8 Word: 0.018*\"custom\" + 0.016*\"product\" + 0.015*\"servic\" + 0.011*\"market\" + 0.009*\"requir\" + 0.008*\"revenu\" + 0.008*\"signific\" + 0.007*\"stock\" + 0.007*\"technolog\" + 0.007*\"increas\" <br><br>\n",
    "\n",
    "And this makes sense seeing as SciQuest describes themselves as \"The SciQuest Supplier Network is a SaaS communications hub that enables efficient and automated transaction interactions between our customers and their suppliers through our procurement, accounts payable and supplier management solutions.\". So their revenue is derived from a subscription based service model. E.g., \"custom\",\"product\",\"servic\", etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, there's certainly tractability here. One could tune the model to identify particular phrases of known interest or could use the model to look for new topics of interest. A potential strategy here is automatically classify firms by topic likihoods and then weight the topics by the historical risk-adjusted returns. From there you could construct an expected score of each topic. Then if a company is later classified (once trained, this be done the moment a new SEC document is posted) into topics with high scores, you'd then have good reason to further examine it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
